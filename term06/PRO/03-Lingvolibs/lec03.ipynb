{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635ecdba-a012-4151-a2e2-a8e5886d822c",
   "metadata": {},
   "source": [
    "#### NLTK\n",
    "\n",
    "NLTK - одна из самых первых библиотек, предназначенных для решения задач NLP; она огромная и содержит очень много разных инструментов, некоторые из них никак не связаны между собой (в отличие от современных библиотек, которые скоро посмотрим). NLTK - больше исследовательская библиотека, конструктор своего рода. Для NLTK есть учебник, написанный авторами: [NLTK book](https://www.nltk.org/book/). Для этого учебника специально существует подмодуль book, который обычно импортируется целиком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eca090-1649-4e59-8c9e-12bb37969c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4929e-e137-44d0-9353-407085c1eae8",
   "metadata": {},
   "source": [
    "В этом модуле есть некий набор текстов и набор предложений, с которыми можно поиграться. \n",
    "\n",
    "Центральный объект для NLTK (по крайней мере, при работе с корпусами) - это Text (nltk.text.Text). По сути, в этом объекте содержится сам текст в виде списка токенов, но у него есть дополнительные методы. Что можно делать с объектом класса Text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517e77cd-2c2a-4b89-b21d-6148f19131aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 84 matches:\n",
      "[ Moby Dick by Herman Melville 1851 ] ETYMOLOGY . ( Su\n",
      "hat white whale must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye k\n",
      " must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye know the white w\n",
      "ib in a squall . Death and devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \n",
      " devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \" Captain Ahab ,\" said Sta\n"
     ]
    }
   ],
   "source": [
    "text1.concordance('Moby', width=100, lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835718f6-a9f0-4cb0-895c-396194162283",
   "metadata": {},
   "source": [
    "Конкорданс ищет первые n вхождений заданного слова с шириной контекста width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e1167d-2a45-4d31-8bed-14380a5431ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship boat sea time captain world man deck pequod other whales air\n",
      "water head crew line thing side way body\n"
     ]
    }
   ],
   "source": [
    "text1.similar('whale', num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0614c9c6-9e72-4c68-8400-05824a0f7e5c",
   "metadata": {},
   "source": [
    "similar возвращает num слов, которые встречаются в похожих контекстах (дистрибутивная похожесть). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435b89e0-d3ad-4fe7-a2b0-14842732d984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_s the_and the_is the_in the_the the_as the_was the_which the_i\n",
      "a_in the_has the_when the_had the_with the_to the_by the_so the_that\n",
      "the_would the_a\n"
     ]
    }
   ],
   "source": [
    "text1.common_contexts(['whale', 'ship'])  # тоже можно задать параметр num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24cc395-0738-4ffb-8eab-f5a31272a542",
   "metadata": {},
   "source": [
    "common_contexts ищет те самые совпадающие контексты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f93190f-8a25-4f8e-8ffc-ebeda81093d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count('Moby')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e450dc-42e6-4f33-b37f-6aa69b6b912c",
   "metadata": {},
   "source": [
    "Можно посчитать количество вхождений какого-то слова. Кстати, к текстам можно применять обычные функции len(), set() и подобные. И срезы с индексами работают!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c5a88a4-1a4d-4879-9d16-9bf4d540b94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long , from one to the top - mast , and no coffin and went out a sea\n",
      "captain -- this peaking of the whales . , so as to preserve all his\n",
      "might had in former years abounding with them , they toil with their\n",
      "lances , strange tales of Southern whaling . at once the bravest\n",
      "Indians he was , after in vain strove to pierce the profundity . ?\n",
      "then ?\" a levelled flame of pale , And give no chance , watch him ;\n",
      "though the line , it is to be gainsaid . have been\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'long , from one to the top - mast , and no coffin and went out a sea\\ncaptain -- this peaking of the whales . , so as to preserve all his\\nmight had in former years abounding with them , they toil with their\\nlances , strange tales of Southern whaling . at once the bravest\\nIndians he was , after in vain strove to pierce the profundity . ?\\nthen ?\" a levelled flame of pale , And give no chance , watch him ;\\nthough the line , it is to be gainsaid . have been'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.generate(length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e03fba-b913-4e78-8aca-d82852fc8a59",
   "metadata": {},
   "source": [
    "Можно сгенерировать текст, \"похожий\" на оригинальный. Это делается с помощью n-грамов (или n-грамм, я видела разные варианты по-русски...): nltk просто в случайном порядке совмещает эти n-грамы. Как можно видеть, не слишком полезный метод, однако можно побаловаться. \n",
    "\n",
    "Важнее то, что в nltk есть утилиты для работы с n-грамами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cee6663-cce0-4cf5-bc4f-a5a27f4d7a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THE', 'suburb', 'of'),\n",
       " ('suburb', 'of', 'Saffron'),\n",
       " ('of', 'Saffron', 'Park'),\n",
       " ('Saffron', 'Park', 'lay'),\n",
       " ('Park', 'lay', 'on'),\n",
       " ('lay', 'on', 'the'),\n",
       " ('on', 'the', 'sunset'),\n",
       " ('the', 'sunset', 'side'),\n",
       " ('sunset', 'side', 'of'),\n",
       " ('side', 'of', 'London'),\n",
       " ('of', 'London', ','),\n",
       " ('London', ',', 'as'),\n",
       " (',', 'as', 'red'),\n",
       " ('as', 'red', 'and'),\n",
       " ('red', 'and', 'ragged'),\n",
       " ('and', 'ragged', 'as'),\n",
       " ('ragged', 'as', 'a'),\n",
       " ('as', 'a', 'cloud'),\n",
       " ('a', 'cloud', 'of'),\n",
       " ('cloud', 'of', 'sunset'),\n",
       " ('of', 'sunset', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams # bigrams\n",
    "\n",
    "list(ngrams(sent9, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c184d-120a-4505-8a7f-cb6455a2d869",
   "metadata": {},
   "source": [
    "Функция ngrams (или bigrams) возвращает список всех н-грамов списка токенов, который ей дать. N-грамы еще принимают число n. \n",
    "\n",
    "У класса Text есть метод, который возвращает коллокации (частотные н-грамы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ea63f7-9ebc-401c-826c-10c237a1ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e0bec-7264-4cc8-abdc-00ab6643df20",
   "metadata": {},
   "source": [
    "Как создать собственный объект класса Text? Достаточно токенизировать свой текст (любым токенизатором) и передать его в класс:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2892a-8538-4c68-840a-3c0325abb865",
   "metadata": {},
   "source": [
    "mytext = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273e73c-ff4a-4124-8688-d509c682cd5a",
   "metadata": {},
   "source": [
    "Гораздо чаще на практике, однако, используются какие-то отдельные инструменты NLTK, предназначенные для обработки текста. \n",
    "\n",
    "Базовая задача, которую, как правило, необходимо решать примерно всегда при работе с текстами - это сегментация текста. Текст можно сегментировать на предложения и/или на токены (существует также отдельная задача сегментации на морфемы, или автоматический глоссинг, но это более редкая вещь). \r\n",
    "\r\n",
    "Поскольку разделять текст на слова и предложения вроде бы очень легко, как правило, используются инструменты на правилах; хотя и не для каждого языка это верно: для языков, у которых слова не отделяются пробелами, токенизаторы могут быть нейронны или использовать статистические алгоритмы: например, для токенизации японского языка используется обычно словарь и алгоритм Витерби.и. Кстати, неплохо вспомнить, что мы сами не всегда знаем, что мы хотим считать за слово. Что мы хотим считать за токен - другой вопрос: токен определяется задачами. Например, для определенной задачи мы можем хотеть считать, что \"бледно-зеленый\" - это два отдельных слова, а для другой - нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091659f6-6fd3-4587-98f1-99661b2a89d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Буллом', 'со', 'ммани', 'мандинги', 'один', 'из', 'атлантических', 'языков', 'нигеро', 'конголезской', 'макросемьи', 'Распространён', 'в', 'прибрежных', 'районах', 'возле', 'границы', 'между', 'Гвинеей', 'и', 'Сьерра', 'Леоне', 'По', 'данным', 'справочника', 'Ethnologue', 'число', 'носителей', 'составляет', '8350', 'человек', 'в', 'Сьерра', 'Леоне', 'и', 'несколько', 'человек', 'в', 'Гвинее', 'другие', 'источники', 'сообщают', 'о', 'гораздо', 'меньшем', 'количестве', 'носителей', 'около', '500', 'чел', 'Наиболее', 'близкородственный', 'язык', 'бом', 'имеется', 'небольшая', 'взаимопонимаемость', 'с', 'шербро', 'Буллом', 'со', 'активно', 'вытесняется', 'соседними', 'языками', 'главным', 'образом', 'темне']\n",
      "['Буллом-со', '(', 'ммани', ',', 'мандинги', ')', '—', 'один', 'из', 'атлантических', 'языков', 'нигеро-конголезской', 'макросемьи', '.', 'Распространён', 'в', 'прибрежных', 'районах', ',', 'возле', 'границы', 'между', 'Гвинеей', 'и', 'Сьерра-Леоне', '.', 'По', 'данным', 'справочника', 'Ethnologue', 'число', 'носителей', 'составляет', '8350', 'человек', 'в', 'Сьерра-Леоне', 'и', 'несколько', 'человек', 'в', 'Гвинее', ',', 'другие', 'источники', 'сообщают', 'о', 'гораздо', 'меньшем', 'количестве', 'носителей', '(', 'около', '500', 'чел', ')', '.', 'Наиболее', 'близкородственный', 'язык', '—', 'бом', ',', 'имеется', 'небольшая', 'взаимопонимаемость', 'с', 'шербро', '.', 'Буллом-со', 'активно', 'вытесняется', 'соседними', 'языками', ',', 'главным', 'образом', '—', 'темне', '.']\n",
      "['Буллом-со (ммани, мандинги) — один из атлантических языков нигеро-конголезской макросемьи.', 'Распространён в прибрежных районах, возле границы между Гвинеей и Сьерра-Леоне.', 'По данным справочника Ethnologue число носителей составляет 8350 человек в Сьерра-Леоне и несколько человек в Гвинее, другие источники сообщают о гораздо меньшем количестве носителей (около 500 чел).', 'Наиболее близкородственный язык — бом, имеется небольшая взаимопонимаемость с шербро.', 'Буллом-со активно вытесняется соседними языками, главным образом — темне.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+')\n",
    "print(tokenizer.tokenize(raw))\n",
    "print(word_tokenize(raw))\n",
    "print(sent_tokenize(raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91a7ff7-41c0-471a-b7b0-0d6fcd21cc67",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Следующий шаг при обработке текстов - это обычно приведение слов к словарной форме. Это уже не такая простая задача, с помощью регулярок ее не решить, приходится использовать либо словари, либо более сложные методы (нейронные сети...)\n",
    "\n",
    "Еще в прошлом веке, правда, придумали (конечно же, для английского языка) более простой способ хоть как-то унифицировать разные словоформы одного слова: стемминг. \n",
    "\n",
    "Стемминг – это уже чисто историческое, можно сказать, явление: в 1980-х, когда еще не было даже графического интерфейса у компьютеров и тем более средств автоматического морфоразбора, Мартин Портер разработал свой алгоритм стемминга: усечение окончания от псевдоосновы. Этот алгоритм так и называется \"стеммер Портера\" и доступен в версиях для нескольких европейских языков, в т.ч. для русского (Snowball – чуть более новая версия). Алгоритм с помощью правил отсекает окончания и суффиксы, основываясь на особенностях языка. Как все правиловое, работает не без ошибок.\n",
    "\n",
    "В NLTK есть несколько стеммеров, а именно:\n",
    "\n",
    "1. PorterStemmer\n",
    "2. SnowballStemmer\n",
    "3. LancasterStemmer\n",
    "4. RegexpStemmer\n",
    "5. RSLPStemmer\n",
    "\n",
    "Все смотреть не будем, можете сами поискать, если интересно, нам хватит двух. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b34f0f-25ca-4b44-8770-f373725d9657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for w in example_words:\n",
    " print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee83afa-02e4-4472-9b85-e75ac1e1581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пердикк\n",
      "не\n",
      "мен\n",
      "десят\n",
      "раз\n",
      "заключа\n",
      "и\n",
      "расторга\n",
      "союз\n",
      "с\n",
      "основн\n",
      "участник\n",
      "войн\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')  # экземпляр класса \n",
    "example = ['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n",
    "for token in example:\n",
    "    print(stemmer.stem(token))  # stem() - метод класса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95297fb-d391-4f2e-9342-a45c297c5f55",
   "metadata": {},
   "source": [
    "Другой, более крутой способ унифицировать словоформы - все-таки приводить их к словарной форме. В NLTK есть WordNetLemmatizer, который ищет нужные леммы в словаре. Работает только для английского. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d2b7a14-0ddd-4d88-9a42-4d5692dca3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "play\n",
      "play\n",
      "playing\n",
      "playing\n",
      "playing\n",
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f62895d-df27-4d60-bf1d-8eef5e2c9c8b",
   "metadata": {},
   "source": [
    "При обработке текстов для решения задач NLP, особенно если используются классические алгоритмы машинного обучения (современные нейронки извлекают признаки из всего, им часто лучше, чтобы текст сохранялся в исходном виде), бывает нужно выкинуть слишком распространенные и малозначимые слова: союзы, предлоги и т.п. В NLTK есть списки таких слов для разных языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "838772bb-8d66-45ec-8815-c5eca548657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стоп-слова: {'зачем', 'бы', 'сам', 'другой', 'есть', 'была', 'много', 'под', 'чуть', 'да', 'при', 'между', 'всегда', 'же', 'сейчас', 'ним', 'тем', 'а', 'по', 'ну', 'эти', 'с', 'было', 'от', 'ей', 'будто', 'один', 'тебя', 'этом', 'какая', 'и', 'через', 'чтоб', 'три', 'вдруг', 'к', 'тот', 'совсем', 'был', 'разве', 'него', 'для', 'хорошо', 'они', 'вас', 'том', 'потому', 'над', 'нельзя', 'быть', 'этот', 'со', 'себе', 'опять', 'всего', 'мне', 'какой', 'ее', 'ничего', 'перед', 'где', 'на', 'когда', 'более', 'меня', 'без', 'о', 'кто', 'мы', 'нас', 'может', 'куда', 'ему', 'эту', 'моя', 'что', 'за', 'лучше', 'чтобы', 'можно', 'им', 'ж', 'во', 'его', 'того', 'этой', 'конечно', 'такой', 'чего', 'них', 'наконец', 'в', 'уж', 'здесь', 'иногда', 'ней', 'вы', 'два', 'еще', 'там', 'после', 'нибудь', 'не', 'из', 'нет', 'ли', 'если', 'никогда', 'этого', 'больше', 'были', 'теперь', 'про', 'до', 'почти', 'раз', 'я', 'так', 'но', 'ни', 'тогда', 'или', 'себя', 'все', 'мой', 'она', 'ведь', 'вот', 'только', 'хоть', 'чем', 'ты', 'об', 'нее', 'свою', 'их', 'он', 'вам', 'тут', 'надо', 'потом', 'у', 'как', 'будет', 'то', 'даже', 'всех', 'всю', 'уже', 'впрочем', 'тоже'}\n",
      "['обработке', 'текстов', 'решения', 'задач', 'NLP', ',', 'особенно', 'используются', 'классические', 'алгоритмы', 'машинного', 'обучения', '(', 'современные', 'нейронки', 'извлекают', 'признаки', ',', 'часто', ',', 'текст', 'сохранялся', 'исходном', 'виде', ')', ',', 'бывает', 'нужно', 'выкинуть', 'слишком', 'распространенные', 'малозначимые', 'слова', ':', 'союзы', ',', 'предлоги', 'т.п', '.', 'NLTK', 'списки', 'таких', 'слов', 'разных', 'языков']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = 'При обработке текстов для решения задач NLP, особенно если используются классические алгоритмы машинного обучения (современные нейронки извлекают признаки из всего, им часто лучше, чтобы текст сохранялся в исходном виде), бывает нужно выкинуть слишком распространенные и малозначимые слова: союзы, предлоги и т.п. В NLTK есть списки таких слов для разных языков'\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "print('Стоп-слова:', stop_words)\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d22bbf-e7d6-42e1-a08a-32bb7621c6bf",
   "metadata": {},
   "source": [
    "Наконец к морфологии. Подробнее о ней мы поговорим в следующие разы, потому что морфологический разбор (с частями речи и еще и грамматическими характеристиками) - еще более сложная задача. В NLTK реализованы простейшие статистические парсеры, которые можно самостоятельно обучать на данных размеченных корпусов NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef0164f1-03db-4623-b72d-e77dbfaaca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "print(nltk.pos_tag(text))\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca0d7cd5-564d-4f6c-aaa4-8b6f7c53c1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', None),\n",
       " ('sentences', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown # Брауновский корпус\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news') # возьмем все размеченные предложения из новостей\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents) # натренируем таггер на размеченных предложениях (он просто посчитает статистику)\n",
    "unigram_tagger.tag(word_tokenize('I want to analyze sentences with NLTK')) # попробуем на предложении, которого он не видел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a404ac1-6720-4a32-9f96-4c25c22b92a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', None),\n",
       " ('sentences', None),\n",
       " ('with', None),\n",
       " ('NLTK', None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "bigram_tagger.tag(word_tokenize('I want to analyze sentences with NLTK'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09dd624-a634-4c0d-813d-73ea567b3699",
   "metadata": {},
   "source": [
    "Bigram tagger работает как будто бы хуже, чем unigram, но в теории он может лучше справляться с омонимией (потому что учитывает рядом стоящее слово, и тогда beautiful book vs book something не разберет одинаково). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c573f-959b-4343-997c-df8ea3b25ad1",
   "metadata": {},
   "source": [
    "Также NLTK умеет работать с данными базы [WordNet](https://wordnet.princeton.edu/). Оттуда он автоматически может извлекать сведения о семантических отношениях между словами, а также на данных WordNet у него реализован алгоритм Леска для решения задачи Word Sense Disambiguation. \n",
    "\n",
    "Это очень известная задача в NLP-мире, можно про нее подробнее посмотреть на [nlpprogress](http://nlpprogress.com/english/word_sense_disambiguation.html). Для ее решения мы должны неоднозначным словам в контекстах сопоставить дефиниции из словаря (в роли какового для английского языка успешно выступает WordNet). Алгоритм Леска был придуман в 1986 году и считается классическим подходом (бейзлайн, ага) для решения этой задачи. Мы предполагаем, что слова в заданном окне контекста (среди окружающих их слов) будут иметь похожую тематику. Это еще называется **дистрибутивная гипотеза** (\"Лингвистические единицы, встречающиеся в схожих контекстах, имеют близкие значения.\", придумали это лингвисты уже много лет назад). По алгоритму Леска, определение в словаре для целевого слова сравнивается со словами, которые стоят вокруг него в контексте. \n",
    "\n",
    "В базовой имплементации алгоритм Леска делает следующее:\n",
    "\n",
    "- считает количество слов, стоящих рядом с искомым словом и оказавшихся в словарном определении слова (для каждого варианта определения)\n",
    "- Каких слов больше всего оказалось, то и значение. \n",
    "\n",
    "Очень просто!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "035e8a28-fefe-4c4a-a215-45b827be09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def get_semantic(seq, key_word):\n",
    "    temp = word_tokenize(seq)\n",
    "    temp = lesk(temp, key_word)\n",
    "    return temp.definition() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "691682e8-a3c8-4844-a0d1-e28ec47cf3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrange for and reserve (something for someone else) in advance\n",
      "a number of sheets (ticket or stamps etc.) bound together on one edge\n"
     ]
    }
   ],
   "source": [
    "print(get_semantic('The table was already booked by someone else', 'book'))\n",
    "print(get_semantic('I love reading books on programming', 'book'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3250870a-fddb-49ff-9536-d61561ee5d6f",
   "metadata": {},
   "source": [
    "Ну и синонимы с антонимами просто собираются напрямую из WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ecda9b-8c50-4e7a-9050-b9eb2962434b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms ['dog.n.01', 'frump.n.01', 'dog.n.03', 'cad.n.01', 'frank.n.02', 'pawl.n.01', 'andiron.n.01', 'chase.v.01']\n",
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syns in wordnet.synsets('dog'):\n",
    " synonyms.append(syns.name())\n",
    "print (\"synonyms\", synonyms)\n",
    "\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    " for l in syn.lemmas():\n",
    "  if l.antonyms():\n",
    "   antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380393b2-4296-4bae-abff-4fb260d5ee49",
   "metadata": {},
   "source": [
    "### Русская морфология и специализированные библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4125b-979e-4a43-92d4-b36eb829eb65",
   "metadata": {},
   "source": [
    "Современные морфопарсеры, как правило, работают на нейронных сетях, но бывают случаи, когда важнее скорость работы, чем качество; тогда используются правиловые морфопарсеры. Для русского языка их два: pymorphy3 и pymystem3. Pymorphy был создан Михаилом Коробовым (вот его известная [статья на хабре](https://habr.com/ru/post/176575/)) как аналог Майстем. Он работает на словаре и использует тагсет [OpenCorpora](http://opencorpora.org/)), а также статистику, предпосчитанную на этом корпусе. \n",
    "Как устроен pymorphy3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "392d7d2e-c084-4d44-bb2f-dc3c01e3bc31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn sing,gent'), normal_form='студентка', score=0.6, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 1),)),\n",
       " Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn plur,nomn'), normal_form='студентка', score=0.4, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 7),))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy3\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "parse = morph.parse('студентки')\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269263a6-5cbc-49d7-a7d0-43e95d75caba",
   "metadata": {},
   "source": [
    "У класса MorphAnalyzer() есть метод parse, который возвращает что? Список экземпляров класса Parse. У этого класса есть свои атрибуты: word (исходная форма слова), tag (грам. инфа), normal_form (лемма), score(предпосчитанная на OpenCorpora вероятность правильности разбора) и несколько технических. \n",
    "\n",
    "Соответственно, получить информацию можно, просто обращаясь к атрибутам (не забудьте, что у нас всегда список, поэтому нужно еще и индекс разбора указывать):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48b3338-03a0-4dca-bc47-0a1cfe117d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "студентки\n",
      "NOUN,anim,femn sing,gent\n",
      "студентка\n"
     ]
    }
   ],
   "source": [
    "print(parse[0].word)\n",
    "print(parse[0].tag)\n",
    "print(parse[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4030be-7b50-49d0-87f7-2b4bfd9b1c06",
   "metadata": {},
   "source": [
    "Атрибут tag &ndash; это экземпляр класса OpencorporaTag, как можно догадаться. У него есть еще свои атрибуты, к которым тоже можно обращаться, чтобы получать более конкретную информацию о слове. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd66885-eaa7-4021-871a-27869c5d83d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: NOUN\n",
      "Одушевленность: anim\n",
      "Падеж: nomn\n",
      "Род: masc\n",
      "Наклонение: None\n",
      "Число: sing\n",
      "Лицо: None\n",
      "Время: None\n",
      "Переходность: None\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('участник')\n",
    "\n",
    "t = parse[0].tag  # я записала в переменную, просто чтобы не копировать каждый раз все целиком\n",
    "# но это то же самое, что parse[0].tag.animacy...\n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\nНаклонение: {t.mood}\\\n",
    "\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb789333-cc89-4ef2-b6f4-16ec1571105f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: VERB\n",
      "Одушевленность: None\n",
      "Падеж: None\n",
      "Род: None\n",
      "Наклонение: indc\n",
      "Число: sing\n",
      "Лицо: 3per\n",
      "Время: pres\n",
      "Переходность: tran\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('говорит')\n",
    "\n",
    "t = parse[0].tag  \n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\n\\\n",
    "Наклонение: {t.mood}\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba8d51-b265-486c-9d16-f437f1fce220",
   "metadata": {},
   "source": [
    "Если вы запрашиваете категорию, которой у данного слова нет (ну нет переходности у существительного), вернется None. \n",
    "\n",
    "Также можно попросить pymorphy поставить слово в конкретную форму или вообще вернуть всю парадигму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1113f149-df57-499a-ba88-5acaca6c6724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='говорят', tag=OpencorporaTag('VERB,impf,tran plur,3per,pres,indc'), normal_form='говорить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'говорят', 415, 6),))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].inflect({'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5e2c99e-3881-4949-a2b8-3881bb106379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='участник', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участник', 2, 0),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 1),)),\n",
       " Parse(word='участнику', tag=OpencorporaTag('NOUN,anim,masc sing,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнику', 2, 2),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 3),)),\n",
       " Parse(word='участником', tag=OpencorporaTag('NOUN,anim,masc sing,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участником', 2, 4),)),\n",
       " Parse(word='участнике', tag=OpencorporaTag('NOUN,anim,masc sing,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнике', 2, 5),)),\n",
       " Parse(word='участники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участники', 2, 6),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 7),)),\n",
       " Parse(word='участникам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участникам', 2, 8),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 9),)),\n",
       " Parse(word='участниками', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниками', 2, 10),)),\n",
       " Parse(word='участниках', tag=OpencorporaTag('NOUN,anim,masc plur,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниках', 2, 11),))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].lexeme  \n",
    "# парадигму глагола лучше не выводить - она длинная; я перед запуском этой ячейки перезапустила разбор с существительным, поэтому не удивляйтесь. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264cc699-121c-4174-a68f-7ec79b447744",
   "metadata": {},
   "source": [
    "Наконец, можно попросить pymorphy выводить грам. информацию по-русски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2455fec0-23d6-4324-bd79-89b0505f3992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СУЩ,од,мр ед,им'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53817080-4326-478f-929f-67020522a39b",
   "metadata": {},
   "source": [
    "Pymorphy очень быстро работает и имеет много возможностей, но совершенно не умеет разрешать омонимию и никак не учитывает контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57972d1-61ce-4587-bdc9-517bbe8d9a0b",
   "metadata": {},
   "source": [
    "Алгоритм, легший в основу Mystem, разрабатывался в ИППИ и был первым вообще для русского языка; его в свое время купил у них Илья Сегалович, доработал, опубликовал собственную статью. Поисковик Яндекса когда-то работал на майстеме. Сам парсер написан в С (для скорости: бинарный поиск в питоне реализовать можно только с внешними библиотеками на С, а у майстема 2 словаря, по которым нужно искать). Для питона под него сделана оболочка (pymystem3). Майстем капризный, тяжело запускается, имеет не так много функций, но работает тоже довольно быстро и умеет доносить на бастардов: сообщать, что слово не найдено в его словаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a55d7bb-b217-42cb-adfd-69f3bdfd0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "\n",
    "m = pymystem3.Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef4208-53bd-4c33-8432-3f600321f1a9",
   "metadata": {},
   "source": [
    "Майстем принимает только сырой текст в виде одной строки: у него встроенный токенизатор, потому что он пытается учитывать контекст. Поэтому стоит указывать entire_input=False при создании экземпляра класса, чтобы он не выводил вообще все, включая пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e4dae4-4ce8-4839-8c2b-53f75f01210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = '''Пердикка II (др.-греч. Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э. После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка. На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере. На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах. С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом. Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5592802-e0c2-4af5-a802-59ac04f739f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = m.lemmatize(raw)\n",
    "analysis = m.analyze(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b1a0655-34e9-4cad-bb85-55ff4bb80afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пердикк',\n",
       " 'II',\n",
       " 'др',\n",
       " 'греча',\n",
       " 'Περδίκκας',\n",
       " 'Β',\n",
       " 'της',\n",
       " 'Μακεδονίας',\n",
       " 'македонский',\n",
       " 'царь']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[:10]  # надеюсь, греча вас тоже порадовала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e996a251-d201-49bc-9ce2-470011ac49f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'пердикк',\n",
       "    'qual': 'bastard',\n",
       "    'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Пердикка'},\n",
       " {'analysis': [], 'text': 'II'},\n",
       " {'analysis': [{'lex': 'др', 'gr': 'S,сокр,мн,неод=(пр|вин|дат|род|твор|им)'}],\n",
       "  'text': 'др'},\n",
       " {'analysis': [{'lex': 'греча', 'gr': 'S,жен,неод=род,мн'}], 'text': 'греч'},\n",
       " {'analysis': [], 'text': 'Περδίκκας'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70dfcb-cd60-46e7-afe1-8c5c709011d1",
   "metadata": {},
   "source": [
    "С леммами вроде все должно быть понятно, а что зашито в анализе?\n",
    "\n",
    "Майстем возвращает список. Каждый токен в этом списке - это словарь с ключами analysis & text. Первого ключа может не быть: если у нас знак пунктуации. Если же он есть, то в нем содержится список (обычно состоящий из одного элемента - если не указать при создании экземпляра класса Mystem glue_grammar_info=False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68dc3e18-6c20-405e-b6b9-c70a5cdb5671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово: {'analysis': [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}], 'text': 'Пердикка'}\n",
      "Его грам. инфа: [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}]\n",
      "Его оригинальная форма: Пердикка\n",
      "Какие есть ключи в словаре с разбором: dict_keys(['lex', 'qual', 'gr'])\n",
      "Лемма: пердикк\n",
      "Этот ключ бывает только тогда, когда слова нет в словаре: bastard\n",
      "А это грам. информация: S,имя,муж,од=(вин,ед|род,ед)\n"
     ]
    }
   ],
   "source": [
    "print(f'Первое слово: {analysis[0]}')\n",
    "print(f\"Его грам. инфа: {analysis[0]['analysis']}\\nЕго оригинальная форма: {analysis[0]['text']}\")\n",
    "print(f\"Какие есть ключи в словаре с разбором: {analysis[0]['analysis'][0].keys()}\")\n",
    "print(f\"Лемма: {analysis[0]['analysis'][0]['lex']}\\n\\\n",
    "Этот ключ бывает только тогда, когда слова нет в словаре: {analysis[0]['analysis'][0]['qual']}\\n\\\n",
    "А это грам. информация: {analysis[0]['analysis'][0]['gr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf56d686-6be4-4db9-84c3-8f90dd57869d",
   "metadata": {},
   "source": [
    "Непросто, да. Еще сложнее устроен ключ 'gr', который содержит грамматическую информацию о слове: обычно майстем склеивает варианты разбора, то есть, выше запись следует читать как \"существительное, имя собственное, мужского рода, одушевленное; возможно, Acc Sg, а возможно, Gen Sg. \n",
    "\n",
    "Вот как раз если указать, чтобы грам. информация не склеивалась, майстем будет возвращать несколько словарей с вариантами по отдельности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59463324-896c-418f-bc1b-171f09d4ec83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis': [{'lex': 'пердикк',\n",
       "   'qual': 'bastard',\n",
       "   'gr': 'S,имя,муж,од=вин,ед'},\n",
       "  {'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=род,ед'}],\n",
       " 'text': 'Пердикка'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_noglue = pymystem3.Mystem(entire_input=False, glue_grammar_info=False)\n",
    "\n",
    "noglueanalysis = m_noglue.analyze(raw)\n",
    "noglueanalysis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e9226e-bbeb-4076-b5d8-a3a9543c6f93",
   "metadata": {},
   "source": [
    "Теперь о вещах, которых нет в стабильной версии Mystem, а есть только в той, которая устанавливается через git:\n",
    "\n",
    "1. Майстем очень плохо умеет обрабатывать \\n. Когда он получает строку, в которой много \\n (а это неизбежно, ведь мы чаще хотим обрабатывать длиннющие тексты), на каждом \\n он перезапускает свой бинарник (написанный в С). Поэтому на длинных текстах работать будет ОЧЕНЬ медленно (впрочем, все равно быстрее нейронок...). Чтобы решить эту проблему - ведь замена \\n на пробелы, например, искажает контекст - сделали возможность особым образом обрабатывать \\n, когда загружаем текст из файла. \n",
    "2. Есть функция, которая позволяет получить часть речи для конкретного токена. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa563c-5040-41f6-94f1-5e1d71f81354",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = m.analyze(file_path=path) # можно напрямую передавать в майстем путь к файлу с текстом - он сам откроет и обработает как ему надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1551734-59df-4d94-a078-76ca2c8b4e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_pos(analysis[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef566446-55cd-40e6-838b-3f60d011f593",
   "metadata": {},
   "source": [
    "### Universal Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d65d8-a929-47f5-9e72-20fd87942396",
   "metadata": {},
   "source": [
    "[Universal Dependencies](https://universaldependencies.org/) - это грандиозный существующий с 2006 года проект (сперва чешских, а потом и самых разных лингвистов, у нас в России им активно занимается О. Ляшевская), который ставит целью разработать такой формат морфосинтаксической разметки, который был бы одинаково применим к самым разным языкам. То есть, основная его фича - это *единообразие*, из-за чего, к примеру, принято решение в русском языке частицу \"не\" считать advmod (так она себя ведет в германских языках...), а копулу не считать вершиной (потому что копула в агглютинативных языках обычно отсутствует, ср. русское \"Петя был учителем\" vs турецкое \"Petya öğretmendi\", где -di - показатель прошедшего времени, присоединяющийся к *существительному*). \n",
    "\n",
    "UD для разметки использует формат файлов .conllu, которые представляют собой таблички (мы с таким на прошлых семинарах имели дело уже). В этом формате существует 10 колонок, каждая ячейка в строке отделяется знаком табуляции; предложения разделяются пустой строкой. На самом сайте UD очень много полезной информации, в том числе описание этого формата и сборник ссылок на приложения, которыми его удобно читать!\n",
    "\n",
    "Существуют готовые библиотеки для чтения в этом формате: pyconll и conllu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810f40b1-299a-4552-b56e-f0565f64b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "text = pyconll.load_from_file('myfile.conllu')\n",
    "\n",
    "for sentence in text:\n",
    "    for token in sentence:\n",
    "        print(token.id, token.form, token.lemma, token.upos, token.feats, token.head, token.deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab43e7a-b21d-4a9e-95ea-b42b6186e679",
   "metadata": {},
   "source": [
    "Естественно, можно не только печатать информацию, но и добавлять в список и вообще делать все, что угодно. Что это за атрибуты у токенов?\n",
    "\n",
    "- id - порядковый номер токена в предложении\n",
    "- form - исходная форма\n",
    "- lemma - лемма\n",
    "- upos - часть речи в UD\n",
    "- xpos - часть речи в неуниверсальном формате (обычно встречается, если датасет конвертированный)\n",
    "- feats - грам. характеристики\n",
    "- head - расстояние от синтаксической вершины\n",
    "- deprel - тип синтаксической связи\n",
    "- две зарезервированные ячейки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ab9b1d-dda9-44e3-9b14-4d012dd1168a",
   "metadata": {},
   "source": [
    "conllu тоже устроен очень просто:\n",
    "\n",
    "    pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "686f261d-2bf1-4dcd-ba07-14f67ddbae61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“While much of the digital transition is unprecedented in the United States, the peaceful transition of power is not,” Obama special assistant Kori Schulman wrote in a blog post Monday.\n",
      "1\t“\t“\tPUNCT\t``\tNone\t20\tpunct\t[('punct', 20)]\t{'SpaceAfter': 'No'}\n",
      "2\tWhile\twhile\tSCONJ\tIN\tNone\t9\tmark\t[('mark', 9)]\tNone\n",
      "3\tmuch\tmuch\tADJ\tJJ\t{'Degree': 'Pos'}\t9\tnsubj\t[('nsubj', 9)]\tNone\n",
      "4\tof\tof\tADP\tIN\tNone\t7\tcase\t[('case', 7)]\tNone\n",
      "5\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t7\tdet\t[('det', 7)]\tNone\n",
      "6\tdigital\tdigital\tADJ\tJJ\t{'Degree': 'Pos'}\t7\tamod\t[('amod', 7)]\tNone\n",
      "7\ttransition\ttransition\tNOUN\tNN\t{'Number': 'Sing'}\t3\tnmod\t[('nmod:of', 3)]\tNone\n",
      "8\tis\tbe\tAUX\tVBZ\t{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\t9\tcop\t[('cop', 9)]\tNone\n",
      "9\tunprecedented\tunprecedented\tADJ\tJJ\t{'Degree': 'Pos'}\t20\tadvcl\t[('advcl:while', 20)]\tNone\n",
      "10\tin\tin\tADP\tIN\tNone\t13\tcase\t[('case', 13)]\tNone\n",
      "11\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t13\tdet\t[('det', 13)]\tNone\n",
      "12\tUnited\tUnited\tPROPN\tNNP\t{'Number': 'Sing'}\t13\tcompound\t[('compound', 13)]\tNone\n",
      "13\tStates\tStates\tPROPN\tNNPS\t{'Number': 'Plur'}\t9\tobl\t[('obl:in', 9)]\t{'SpaceAfter': 'No'}\n",
      "14\t,\t,\tPUNCT\t,\tNone\t20\tpunct\t[('punct', 20)]\tNone\n",
      "15\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t17\tdet\t[('det', 17)]\tNone\n",
      "16\tpeaceful\tpeaceful\tADJ\tJJ\t{'Degree': 'Pos'}\t17\tamod\t[('amod', 17)]\tNone\n",
      "17\ttransition\ttransition\tNOUN\tNN\t{'Number': 'Sing'}\t20\tnsubj\t[('nsubj', 20)]\tNone\n",
      "18\tof\tof\tADP\tIN\tNone\t19\tcase\t[('case', 19)]\tNone\n",
      "19\tpower\tpower\tNOUN\tNN\t{'Number': 'Sing'}\t17\tnmod\t[('nmod:of', 17)]\tNone\n",
      "20\tis\tbe\tAUX\tVBZ\t{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\t0\troot\t[('root', 0)]\tNone\n",
      "21\tnot\tnot\tADV\tRB\t{'Polarity': 'Neg'}\t20\tadvmod\t[('advmod', 20)]\t{'SpaceAfter': 'No'}\n",
      "22\t,\t,\tPUNCT\t,\tNone\t20\tpunct\t[('punct', 20)]\t{'SpaceAfter': 'No'}\n",
      "23\t”\t”\tPUNCT\t''\tNone\t20\tpunct\t[('punct', 20)]\tNone\n",
      "24\tObama\tObama\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tcompound\t[('compound', 26)]\tNone\n",
      "25\tspecial\tspecial\tADJ\tJJ\t{'Degree': 'Pos'}\t26\tamod\t[('amod', 26)]\tNone\n",
      "26\tassistant\tassistant\tNOUN\tNN\t{'Number': 'Sing'}\t29\tnsubj\t[('nsubj', 29)]\tNone\n",
      "27\tKori\tKori\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tflat\t[('flat', 26)]\tNone\n",
      "28\tSchulman\tSchulman\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tflat\t[('flat', 26)]\tNone\n",
      "29\twrote\twrite\tVERB\tVBD\t{'Mood': 'Ind', 'Tense': 'Past', 'VerbForm': 'Fin'}\t20\tparataxis\t[('parataxis', 20)]\tNone\n",
      "30\tin\tin\tADP\tIN\tNone\t33\tcase\t[('case', 33)]\tNone\n",
      "31\ta\ta\tDET\tDT\t{'Definite': 'Ind', 'PronType': 'Art'}\t33\tdet\t[('det', 33)]\tNone\n",
      "32\tblog\tblog\tNOUN\tNN\t{'Number': 'Sing'}\t33\tcompound\t[('compound', 33)]\tNone\n",
      "33\tpost\tpost\tNOUN\tNN\t{'Number': 'Sing'}\t29\tobl\t[('obl:in', 29)]\tNone\n",
      "34\tMonday\tMonday\tPROPN\tNNP\t{'Number': 'Sing'}\t29\tnmod:tmod\t[('nmod:tmod', 29)]\t{'SpaceAfter': 'No'}\n",
      "35\t.\t.\tPUNCT\t.\tNone\t20\tpunct\t[('punct', 20)]\tNone\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse # парсит одиночное предложение, загружает все в оперативную память\n",
    "from conllu import parse_incr # загружает в память предложения по очереди\n",
    "\n",
    "with open(\"en_pud-ud-test.conllu\", \"r\", encoding=\"utf-8\") as datafile:\n",
    "    data = datafile.read()\n",
    "    sentences = parse(data) # data - строка, содержащая разметку; возвращает специальный объект, у которого есть список токенов и исходный текст\n",
    "print(sentences[0].metadata['text']) # в атрибуте metadata содержатся метаданные о предложении: его sent_id, text, возможно, перевод (если есть)\n",
    "for token in sentences[0]:\n",
    "    print(token['id'], token['form'], token['lemma'], token['upos'], token['xpos'], token['feats'], token['head'], token['deprel'], token['deps'], token['misc'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "174360a7-9587-4e60-b1a1-2aca1523cbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“While much of the digital transition is unprecedented in the United States, the peaceful transition of power is not,” Obama special assistant Kori Schulman wrote in a blog post Monday.\n",
      "1\t“\t“\tPUNCT\t``\tNone\t20\tpunct\t[('punct', 20)]\t{'SpaceAfter': 'No'}\n",
      "2\tWhile\twhile\tSCONJ\tIN\tNone\t9\tmark\t[('mark', 9)]\tNone\n",
      "3\tmuch\tmuch\tADJ\tJJ\t{'Degree': 'Pos'}\t9\tnsubj\t[('nsubj', 9)]\tNone\n",
      "4\tof\tof\tADP\tIN\tNone\t7\tcase\t[('case', 7)]\tNone\n",
      "5\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t7\tdet\t[('det', 7)]\tNone\n",
      "6\tdigital\tdigital\tADJ\tJJ\t{'Degree': 'Pos'}\t7\tamod\t[('amod', 7)]\tNone\n",
      "7\ttransition\ttransition\tNOUN\tNN\t{'Number': 'Sing'}\t3\tnmod\t[('nmod:of', 3)]\tNone\n",
      "8\tis\tbe\tAUX\tVBZ\t{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\t9\tcop\t[('cop', 9)]\tNone\n",
      "9\tunprecedented\tunprecedented\tADJ\tJJ\t{'Degree': 'Pos'}\t20\tadvcl\t[('advcl:while', 20)]\tNone\n",
      "10\tin\tin\tADP\tIN\tNone\t13\tcase\t[('case', 13)]\tNone\n",
      "11\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t13\tdet\t[('det', 13)]\tNone\n",
      "12\tUnited\tUnited\tPROPN\tNNP\t{'Number': 'Sing'}\t13\tcompound\t[('compound', 13)]\tNone\n",
      "13\tStates\tStates\tPROPN\tNNPS\t{'Number': 'Plur'}\t9\tobl\t[('obl:in', 9)]\t{'SpaceAfter': 'No'}\n",
      "14\t,\t,\tPUNCT\t,\tNone\t20\tpunct\t[('punct', 20)]\tNone\n",
      "15\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t17\tdet\t[('det', 17)]\tNone\n",
      "16\tpeaceful\tpeaceful\tADJ\tJJ\t{'Degree': 'Pos'}\t17\tamod\t[('amod', 17)]\tNone\n",
      "17\ttransition\ttransition\tNOUN\tNN\t{'Number': 'Sing'}\t20\tnsubj\t[('nsubj', 20)]\tNone\n",
      "18\tof\tof\tADP\tIN\tNone\t19\tcase\t[('case', 19)]\tNone\n",
      "19\tpower\tpower\tNOUN\tNN\t{'Number': 'Sing'}\t17\tnmod\t[('nmod:of', 17)]\tNone\n",
      "20\tis\tbe\tAUX\tVBZ\t{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\t0\troot\t[('root', 0)]\tNone\n",
      "21\tnot\tnot\tADV\tRB\t{'Polarity': 'Neg'}\t20\tadvmod\t[('advmod', 20)]\t{'SpaceAfter': 'No'}\n",
      "22\t,\t,\tPUNCT\t,\tNone\t20\tpunct\t[('punct', 20)]\t{'SpaceAfter': 'No'}\n",
      "23\t”\t”\tPUNCT\t''\tNone\t20\tpunct\t[('punct', 20)]\tNone\n",
      "24\tObama\tObama\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tcompound\t[('compound', 26)]\tNone\n",
      "25\tspecial\tspecial\tADJ\tJJ\t{'Degree': 'Pos'}\t26\tamod\t[('amod', 26)]\tNone\n",
      "26\tassistant\tassistant\tNOUN\tNN\t{'Number': 'Sing'}\t29\tnsubj\t[('nsubj', 29)]\tNone\n",
      "27\tKori\tKori\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tflat\t[('flat', 26)]\tNone\n",
      "28\tSchulman\tSchulman\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tflat\t[('flat', 26)]\tNone\n",
      "29\twrote\twrite\tVERB\tVBD\t{'Mood': 'Ind', 'Tense': 'Past', 'VerbForm': 'Fin'}\t20\tparataxis\t[('parataxis', 20)]\tNone\n",
      "30\tin\tin\tADP\tIN\tNone\t33\tcase\t[('case', 33)]\tNone\n",
      "31\ta\ta\tDET\tDT\t{'Definite': 'Ind', 'PronType': 'Art'}\t33\tdet\t[('det', 33)]\tNone\n",
      "32\tblog\tblog\tNOUN\tNN\t{'Number': 'Sing'}\t33\tcompound\t[('compound', 33)]\tNone\n",
      "33\tpost\tpost\tNOUN\tNN\t{'Number': 'Sing'}\t29\tobl\t[('obl:in', 29)]\tNone\n",
      "34\tMonday\tMonday\tPROPN\tNNP\t{'Number': 'Sing'}\t29\tnmod:tmod\t[('nmod:tmod', 29)]\t{'SpaceAfter': 'No'}\n",
      "35\t.\t.\tPUNCT\t.\tNone\t20\tpunct\t[('punct', 20)]\tNone\n"
     ]
    }
   ],
   "source": [
    "data_file = open(\"en_pud-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "for i, sentence in enumerate(parse_incr(data_file)):\n",
    "    if i != 0:\n",
    "        break\n",
    "    print(sentence.metadata['text'])\n",
    "    for token in sentence:\n",
    "        print(token['id'], token['form'], token['lemma'], token['upos'], token['xpos'], token['feats'], token['head'], token['deprel'], token['deps'], token['misc'], sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118194f7-2fcc-4a69-bfb4-0a8119bf2093",
   "metadata": {},
   "source": [
    "Где можно красивенько отрисовать .conllu файлы в виде деревьев зависимости:\n",
    "\n",
    "[Арборатор](https://arborator.ilpga.fr/q.cgi): достаточно вставить текст в формате .conllu\n",
    "\n",
    "[Conllu-Viewer на сайте UD](https://universaldependencies.org/conllu_viewer.html): умеет читать файлы и рисовать последовательно все предложения\n",
    "\n",
    "Для затравки вот картинка с арборатора:\n",
    "\n",
    "<img src='arbo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddfe729-e1e4-4e50-a68f-8bd6c260f907",
   "metadata": {},
   "source": [
    "Авторы UD разместили онлайн-парсер на своем [сайте](https://lindat.mff.cuni.cz/services/udpipe/), однако для русских он без VPN не работает теперь, к сожалению."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d8c4f-24e9-4a98-b6b0-b7fd08ddee26",
   "metadata": {},
   "source": [
    "## Крупные библиотеки NLP: SpaCy, Stanza (StanfordNLP), Natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae004a6-30b3-4937-ba32-d243c80a3e13",
   "metadata": {},
   "source": [
    "#### SpaCy\n",
    "\n",
    "Спейси - современная библиотека, которая написана в Cython и использует нейронные сети. Интерфейс спейси довольно удобный и однообразный. Центральное понятие для спейси - это pipeline: то есть, набор действий, которые спейси совершает с текстом. Чтобы обработать текст на любом из языков, представленных в библиотеке ([список](https://spacy.io/usage/models)), достаточно завести пустой пайплайн:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "032f8015-7cea-4b60-a2b7-0bf174f47363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp('My beautiful sentence is here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d94e5-9758-4ad2-b5bc-5fa15b96ff2a",
   "metadata": {},
   "source": [
    "Предупреждения может выдавать библиотека tensorflow, которая сообщает, что у вас не настроена CUDA, но их можно игнорировать, как и предлагается. \n",
    "\n",
    "Пустой пайплайн превращает наш текст в особый объект, в котором текст разделен на токены, и можно у этих токенов смотреть самые простые характеристики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78b7330-607e-469b-a303-463c5368bcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "№ 0. Токен:              My. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 1. Токен:       beautiful. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 2. Токен:        sentence. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 3. Токен:              is. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 4. Токен:            here. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 5. Токен:               .. Является словом: False. Является пунктуацией: True. Похож на число: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'№ {token.i}. Токен: {token.text:>15}. Является словом: {token.is_alpha}. Является пунктуацией: {token.is_punct}. Похож на число: {token.like_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63301fa-6ef1-4419-a965-137d23327c5a",
   "metadata": {},
   "source": [
    "Объект doc также позволяет смотреть спаны (несколько токенов, срез текста):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d52033e-8ad1-47c4-9c96-8176ba5e6ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful sentence'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d2ee4-1222-4bd1-9d73-92fda25667f8",
   "metadata": {},
   "source": [
    "В стандартный пайплайн спейси сентенизация не входит, но можно ее добавить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09a52e15-1b4f-47ae-a171-04a2b502cbb5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Буллом', '-', 'со', '(', 'ммани', ',', 'мандинги', ')', '—', 'один', 'из', 'атлантических', 'языков', 'нигеро', '-', 'конголезской', 'макросемьи', '.', 'Распространён', 'в', 'прибрежных', 'районах', ',', 'возле', 'границы', 'между', 'Гвинеей', 'и', 'Сьерра', '-', 'Леоне', '.', 'По', 'данным', 'справочника', 'Ethnologue', 'число', 'носителей', 'составляет', '8350', 'человек', 'в', 'Сьерра', '-', 'Леоне', 'и', 'несколько', 'человек', 'в', 'Гвинее', ',', 'другие', 'источники', 'сообщают', 'о', 'гораздо', 'меньшем', 'количестве', 'носителей', '(', 'около', '500', 'чел', ')', '.', 'Наиболее', 'близкородственный', 'язык', '—', 'бом', ',', 'имеется', 'небольшая', 'взаимопонимаемость', 'с', 'шербро', '.', 'Буллом', '-', 'со', 'активно', 'вытесняется', 'соседними', 'языками', ',', 'главным', 'образом', '—', 'темне', '.']\n",
      "['Буллом-со (ммани, мандинги) — один из атлантических языков нигеро-конголезской макросемьи.', 'Распространён в прибрежных районах, возле границы между Гвинеей и Сьерра-Леоне.', 'По данным справочника Ethnologue число носителей составляет 8350 человек в Сьерра-Леоне и несколько человек в Гвинее, другие источники сообщают о гораздо меньшем количестве носителей (около 500 чел).', 'Наиболее близкородственный язык — бом, имеется небольшая взаимопонимаемость с шербро.', 'Буллом-со активно вытесняется соседними языками, главным образом — темне.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('ru')\n",
    "nlp.add_pipe('sentencizer')\n",
    "doc = nlp(raw) # в этот момент спейси предобрабатывает наш сырой текст \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "sents = [sent.text for sent in doc.sents]\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f16b05-5761-4700-a02f-ac6cf23341bd",
   "metadata": {},
   "source": [
    "Ну, это все прекрасно, но хотелось бы чего-то большего. Для spacy есть довольно много предобученных моделей для разных языков (список см. выше). Модельки нужно устанавливать (скачивать) с помощью команды в командной строке - она написана у них на сайте (python -m spacy download имя_модели). Когда вы загрузили модельку, вы можете ее использовать в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49d4b7f0-17e0-4c46-ac88-622b433679e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffd832-84fb-4f51-b3cc-b94748e08f19",
   "metadata": {},
   "source": [
    "Теперь уже можно получить сведения поинтереснее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc41d86-b586-4bda-9c0e-68581ab56df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8b55c4f-fccf-4d48-b79a-5516ba7db775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. The             POS: DET    SyntR: det       Head: Army\n",
      " 1. 4th             POS: ADJ    SyntR: amod      Head: Army\n",
      " 2. Army            POS: PROPN  SyntR: nsubj     Head: was\n",
      " 3. was             POS: AUX    SyntR: ROOT      Head: was\n",
      " 4. a               POS: DET    SyntR: det       Head: formation\n",
      " 5. Royal           POS: PROPN  SyntR: compound  Head: Army\n",
      " 6. Yugoslav        POS: ADJ    SyntR: amod      Head: Army\n",
      " 7. Army            POS: PROPN  SyntR: compound  Head: formation\n",
      " 8. formation       POS: NOUN   SyntR: attr      Head: was\n",
      " 9. mobilised       POS: VERB   SyntR: acl       Head: formation\n",
      "10. prior           POS: ADV    SyntR: advmod    Head: mobilised\n",
      "11. to              POS: ADP    SyntR: prep      Head: prior\n",
      "12. the             POS: DET    SyntR: det       Head: invasion\n",
      "13. German          POS: PROPN  SyntR: npadvmod  Head: led\n",
      "14. -               POS: PUNCT  SyntR: punct     Head: led\n",
      "15. led             POS: VERB   SyntR: amod      Head: invasion\n",
      "16. invasion        POS: NOUN   SyntR: pobj      Head: to\n",
      "17. of              POS: ADP    SyntR: prep      Head: invasion\n",
      "18. the             POS: DET    SyntR: det       Head: Kingdom\n",
      "19. Kingdom         POS: PROPN  SyntR: pobj      Head: of\n",
      "20. of              POS: ADP    SyntR: prep      Head: Kingdom\n",
      "21. Yugoslavia      POS: PROPN  SyntR: pobj      Head: of\n",
      "22. during          POS: ADP    SyntR: prep      Head: mobilised\n",
      "23. World           POS: PROPN  SyntR: compound  Head: II\n",
      "24. War             POS: PROPN  SyntR: compound  Head: II\n",
      "25. II              POS: PROPN  SyntR: pobj      Head: during\n",
      "26. .               POS: PUNCT  SyntR: punct     Head: was\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.i:2}. {token.text:15} POS: {token.pos_:6} SyntR: {token.dep_:9} Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbf18b-7aae-4c8b-9c83-391a1670af2e",
   "metadata": {},
   "source": [
    "Если нужно, чтобы индексы расставлялись так, как это должно быть в UD (нумерация с 1, отдельная индексация в каждом новом предложении), можно использовать такой код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11291e9c-3494-4c33-a350-94757868d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "  for token in sent:\n",
    "    if token.dep_ == 'ROOT':\n",
    "      head = 0\n",
    "    else:\n",
    "      head = token.head.i - sent.start + 1\n",
    "    print(token.i - sent.start + 1, token.text, head, token.morph)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb168247-1454-448d-aebf-b2290cc13ce0",
   "metadata": {},
   "source": [
    "Грамматические характеристики можно тоже посмотреть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a329cc-1112-49e2-8a01-0d430b5dfaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definite=Def|PronType=Art\n",
      "Degree=Pos\n",
      "Number=Sing\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:3]:\n",
    "    print(token.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc907ea-e86d-4dc1-be89-e1049f5cb002",
   "metadata": {},
   "source": [
    "Также spacy позволяет разметить именованные сущности и посмотреть, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c19e586d-1f3e-4b06-a012-cbc5738a092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: The 4th Army                   Label: ORG\n",
      "Entity: Royal Yugoslav Army            Label: ORG\n",
      "Entity: German                         Label: NORP\n",
      "Entity: the Kingdom of Yugoslavia      Label: GPE\n",
      "Entity: World War II                   Label: EVENT\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text:30} Label: {ent.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a2272-1460-4430-b922-69aab7ef79fe",
   "metadata": {},
   "source": [
    "Если аббревиатуры вас смущают, spacy легко предоставит расшифровки (для всех!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bed61d33-475c-4b9d-93d0-51feecddc658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9bdfa66d-ea1f-4ca7-b637-0254a97b661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object of preposition'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('pobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d65060-de13-4fd3-944e-7d061e21418d",
   "metadata": {},
   "source": [
    "Спейски библиотека удобная и самая популярная, с простым синтаксисом, но если хочется использовать оригинальный UDPipe, есть обертки для spacy + udpipe (для тех, кто хочет запускать его локально, а не с сайта). UD, как известно - очень популярный и влиятельный проект (чешский), цель которого - унифицировать тагсет для всех языков мира, причем как в морфологии, так и в синтаксисе. Неудивительно, что, поскольку в UD существует большое количество размеченных датасетов (русским языком активно занималась О. Ляшевская), то они обучили и свой парсер. Сам парсер написан в плюсах и не очень дружелюбен, но обертки для spacy делают жизнь проще. \n",
    "\n",
    "Внимание: я обычно это забываю, но прежде чем установить udpipe, **нужно поставить C++ Build Tools** (с сайта майкрософт, это бесплатно). Это нужно, чтобы скомпилировать udpipe из исходников в сях. Адрес сайта, откуда брать их, сам pip обычно подсказывает, но в целом можно просто погуглить. \n",
    "\n",
    "Две оболочки для udpipe, которые мы рассмотрим - это spacy_udpipe и corpy. Обе нужно устанавливать:\n",
    "\n",
    "    pip install spacy_udpipe\n",
    "    pip install corpy\n",
    "    \n",
    "Для spacy_udpipe вообще ничего больше не нужно, там максимально автоматизированно скачиваются модельки. Для corpy приходится скачивать искомую модель руками, [отсюда](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131) (к сожалению, с некоторых пор тоже открывается только с VPN). Эту модель вы можете положить куда угодно, главное потом указать corpy путь к ней. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "910212bf-8664-4239-be26-ec9a4fc255f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'en' language\n"
     ]
    }
   ],
   "source": [
    "import spacy_udpipe\n",
    "\n",
    "spacy_udpipe.download('en')  # эту команду достаточно выполнить только один раз - она как nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "07811dbe-01ef-4020-b1ac-8cbbcec50dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. The             Lemma: the             POS: DET    SyntR: det       Head: Army\n",
      " 1. 4th             Lemma: 4th             POS: ADJ    SyntR: amod      Head: Army\n",
      " 2. Army            Lemma: Army            POS: PROPN  SyntR: nsubj     Head: formation\n",
      " 3. was             Lemma: be              POS: AUX    SyntR: cop       Head: formation\n",
      " 4. a               Lemma: a               POS: DET    SyntR: det       Head: formation\n",
      " 5. Royal           Lemma: Royal           POS: PROPN  SyntR: compound  Head: Army\n",
      " 6. Yugoslav        Lemma: Yugoslav        POS: PROPN  SyntR: compound  Head: Army\n",
      " 7. Army            Lemma: Army            POS: PROPN  SyntR: compound  Head: formation\n",
      " 8. formation       Lemma: formation       POS: NOUN   SyntR: nsubj     Head: mobilised\n",
      " 9. mobilised       Lemma: mobilise        POS: VERB   SyntR: ROOT      Head: mobilised\n",
      "10. prior           Lemma: prior           POS: ADJ    SyntR: case      Head: invasion\n",
      "11. to              Lemma: to              POS: ADP    SyntR: fixed     Head: prior\n",
      "12. the             Lemma: the             POS: DET    SyntR: det       Head: invasion\n",
      "13. German          Lemma: german          POS: ADJ    SyntR: obl:npmod Head: led\n",
      "14. -               Lemma: -               POS: PUNCT  SyntR: punct     Head: led\n",
      "15. led             Lemma: lead            POS: VERB   SyntR: amod      Head: invasion\n",
      "16. invasion        Lemma: invasion        POS: NOUN   SyntR: obl       Head: mobilised\n",
      "17. of              Lemma: of              POS: ADP    SyntR: case      Head: Kingdom\n",
      "18. the             Lemma: the             POS: DET    SyntR: det       Head: Kingdom\n",
      "19. Kingdom         Lemma: Kingdom         POS: PROPN  SyntR: nmod      Head: invasion\n",
      "20. of              Lemma: of              POS: ADP    SyntR: case      Head: Yugoslavia\n",
      "21. Yugoslavia      Lemma: Yugoslavia      POS: PROPN  SyntR: nmod      Head: Kingdom\n",
      "22. during          Lemma: during          POS: ADP    SyntR: case      Head: II\n",
      "23. World           Lemma: World           POS: PROPN  SyntR: compound  Head: War\n",
      "24. War             Lemma: War             POS: PROPN  SyntR: compound  Head: II\n",
      "25. II              Lemma: ii              POS: PROPN  SyntR: nmod      Head: Kingdom\n",
      "26. .               Lemma: .               POS: PUNCT  SyntR: punct     Head: mobilised\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy_udpipe.load('en')\n",
    "doc = nlp('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')\n",
    "for token in doc:\n",
    "    print(f'{token.i:2}. {token.text:15} Lemma: {token.lemma_:15} POS: {token.pos_:6} SyntR: {token.dep_:9} Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b6943-1ed2-4694-9db2-0d9758accfaa",
   "metadata": {},
   "source": [
    "То же самое можно сделать в corpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5afcb14f-da1e-45e9-8b2e-6ddba2cff109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpy.udpipe import Model\n",
    "\n",
    "model = Model('english-partut-ud-2.5-191206.udpipe')  \n",
    "# тут и нужно указать путь к вашей модели. Если она лежит в той же папке, что и скрипт, достаточно только имени файла\n",
    "\n",
    "sents = model.process('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8751f7-64c1-4e5f-89b3-e8ce1c4b2c94",
   "metadata": {},
   "source": [
    "corpy возвращает генератор (то есть, итерируемый объект, который как магазин автомата, расстреляли все патроны - он опустел; повторно по генератору итерироваться нельзя). Генератор на каждом шаге выдает предложение (объект специального класса Sentence()), а в предложении - объекты класса Word(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d135ba76-3f62-4f59-a552-13e8d4e54020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<root>          Лемма: <root>          POS: <root> Грам. инфа: <root>\n",
      "The             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "4th             Лемма: 4th             POS: ADJ Грам. инфа: Degree=Pos\n",
      "Army            Лемма: army            POS: NOUN Грам. инфа: Number=Sing\n",
      "was             Лемма: be              POS: AUX Грам. инфа: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "a               Лемма: a               POS: DET Грам. инфа: Definite=Ind|Number=Sing|PronType=Art\n",
      "Royal           Лемма: royal           POS: PROPN Грам. инфа: \n",
      "Yugoslav        Лемма: Yugoslav        POS: PROPN Грам. инфа: \n",
      "Army            Лемма: army            POS: PROPN Грам. инфа: \n",
      "formation       Лемма: formation       POS: NOUN Грам. инфа: Number=Sing\n",
      "mobilised       Лемма: mobilize        POS: VERB Грам. инфа: Mood=Ind|Person=3|Tense=Past|VerbForm=Fin\n",
      "prior           Лемма: prior           POS: ADJ Грам. инфа: Degree=Pos\n",
      "to              Лемма: to              POS: ADP Грам. инфа: \n",
      "the             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "German          Лемма: German          POS: PROPN Грам. инфа: \n",
      "-               Лемма: -               POS: PUNCT Грам. инфа: \n",
      "led             Лемма: lead            POS: VERB Грам. инфа: Tense=Past|VerbForm=Part\n",
      "invasion        Лемма: invasion        POS: NOUN Грам. инфа: Number=Sing\n",
      "of              Лемма: of              POS: ADP Грам. инфа: \n",
      "the             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "Kingdom         Лемма: kingdom         POS: NOUN Грам. инфа: Number=Sing\n",
      "of              Лемма: of              POS: ADP Грам. инфа: \n",
      "Yugoslavia      Лемма: Yugoslavia      POS: PROPN Грам. инфа: \n",
      "during          Лемма: during          POS: ADP Грам. инфа: \n",
      "World           Лемма: world           POS: NOUN Грам. инфа: Number=Sing\n",
      "War             Лемма: war             POS: NOUN Грам. инфа: Number=Sing\n",
      "II              Лемма: second          POS: ADJ Грам. инфа: Degree=Pos|NumType=Ord\n",
      ".               Лемма: .               POS: PUNCT Грам. инфа: \n",
      "Алилуя!\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    for word in sent.words:\n",
    "        print(f'{word.form:15} Лемма: {word.lemma:15} POS: {word.upostag} Грам. инфа: {word.feats}')\n",
    "print('Алилуя!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2630088a-9d9a-487e-aa05-b160ad63d4f8",
   "metadata": {},
   "source": [
    "у corpy, кстати, есть свой способ вывода имеющейся информации (хотя, по-моему, в юпитере и так красиво выводится...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8750feb-5ca1-4cde-83c8-0d6a673d3a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\n",
      "   comments=[\n",
      "     '# newdoc',\n",
      "     '# newpar',\n",
      "     '# sent_id = 1',\n",
      "     '# text = The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.'],\n",
      "   words=[\n",
      "     Word(id=0, <root>),\n",
      "     Word(id=1,\n",
      "          form='The',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=3,\n",
      "          deprel='det'),\n",
      "     Word(id=2,\n",
      "          form='4th',\n",
      "          lemma='4th',\n",
      "          xpostag='A',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos',\n",
      "          head=3,\n",
      "          deprel='amod'),\n",
      "     Word(id=3,\n",
      "          form='Army',\n",
      "          lemma='army',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=9,\n",
      "          deprel='nsubj'),\n",
      "     Word(id=4,\n",
      "          form='was',\n",
      "          lemma='be',\n",
      "          xpostag='V',\n",
      "          upostag='AUX',\n",
      "          feats='Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
      "          head=9,\n",
      "          deprel='cop'),\n",
      "     Word(id=5,\n",
      "          form='a',\n",
      "          lemma='a',\n",
      "          xpostag='RI',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Ind|Number=Sing|PronType=Art',\n",
      "          head=9,\n",
      "          deprel='det'),\n",
      "     Word(id=6,\n",
      "          form='Royal',\n",
      "          lemma='royal',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=9,\n",
      "          deprel='nmod'),\n",
      "     Word(id=7,\n",
      "          form='Yugoslav',\n",
      "          lemma='Yugoslav',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=6,\n",
      "          deprel='flat'),\n",
      "     Word(id=8,\n",
      "          form='Army',\n",
      "          lemma='army',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=6,\n",
      "          deprel='flat'),\n",
      "     Word(id=9,\n",
      "          form='formation',\n",
      "          lemma='formation',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='nsubj'),\n",
      "     Word(id=10,\n",
      "          form='mobilised',\n",
      "          lemma='mobilize',\n",
      "          xpostag='V',\n",
      "          upostag='VERB',\n",
      "          feats='Mood=Ind|Person=3|Tense=Past|VerbForm=Fin',\n",
      "          head=0,\n",
      "          deprel='root'),\n",
      "     Word(id=11,\n",
      "          form='prior',\n",
      "          lemma='prior',\n",
      "          xpostag='A',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos',\n",
      "          head=10,\n",
      "          deprel='amod'),\n",
      "     Word(id=12,\n",
      "          form='to',\n",
      "          lemma='to',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=17,\n",
      "          deprel='case'),\n",
      "     Word(id=13,\n",
      "          form='the',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=17,\n",
      "          deprel='det'),\n",
      "     Word(id=14,\n",
      "          form='German',\n",
      "          lemma='German',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=16,\n",
      "          deprel='obl',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=15,\n",
      "          form='-',\n",
      "          lemma='-',\n",
      "          xpostag='FF',\n",
      "          upostag='PUNCT',\n",
      "          head=16,\n",
      "          deprel='punct',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=16,\n",
      "          form='led',\n",
      "          lemma='lead',\n",
      "          xpostag='V',\n",
      "          upostag='VERB',\n",
      "          feats='Tense=Past|VerbForm=Part',\n",
      "          head=17,\n",
      "          deprel='acl'),\n",
      "     Word(id=17,\n",
      "          form='invasion',\n",
      "          lemma='invasion',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='obl'),\n",
      "     Word(id=18,\n",
      "          form='of',\n",
      "          lemma='of',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=20,\n",
      "          deprel='case'),\n",
      "     Word(id=19,\n",
      "          form='the',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=20,\n",
      "          deprel='det'),\n",
      "     Word(id=20,\n",
      "          form='Kingdom',\n",
      "          lemma='kingdom',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=17,\n",
      "          deprel='nmod'),\n",
      "     Word(id=21,\n",
      "          form='of',\n",
      "          lemma='of',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=22,\n",
      "          deprel='case'),\n",
      "     Word(id=22,\n",
      "          form='Yugoslavia',\n",
      "          lemma='Yugoslavia',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=20,\n",
      "          deprel='nmod'),\n",
      "     Word(id=23,\n",
      "          form='during',\n",
      "          lemma='during',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=25,\n",
      "          deprel='case'),\n",
      "     Word(id=24,\n",
      "          form='World',\n",
      "          lemma='world',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=25,\n",
      "          deprel='nmod'),\n",
      "     Word(id=25,\n",
      "          form='War',\n",
      "          lemma='war',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='obl'),\n",
      "     Word(id=26,\n",
      "          form='II',\n",
      "          lemma='second',\n",
      "          xpostag='NO',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos|NumType=Ord',\n",
      "          head=25,\n",
      "          deprel='amod',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=27,\n",
      "          form='.',\n",
      "          lemma='.',\n",
      "          xpostag='FS',\n",
      "          upostag='PUNCT',\n",
      "          head=10,\n",
      "          deprel='punct',\n",
      "          misc='SpaceAfter=No')])]\n"
     ]
    }
   ],
   "source": [
    "from corpy.udpipe import pprint\n",
    "\n",
    "pprint(list(model.process('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28b252-1e35-45f2-b742-d5054208046a",
   "metadata": {
    "id": "5GU9unbnvJXP"
   },
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d2648-5f8b-4934-aa59-1f2792d79db1",
   "metadata": {
    "id": "eKwCF6UOvRCh"
   },
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd9b7d-f55e-4c39-a4ae-11311ef95529",
   "metadata": {
    "id": "Zv7BrtxEvUwY"
   },
   "source": [
    "Загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c84f28d-dab3-4a6d-9f6b-b7604d447be9",
   "metadata": {
    "executionInfo": {
     "elapsed": 2995,
     "status": "ok",
     "timestamp": 1663154625797,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "8UtSgSntvXro"
   },
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411cbec-c9b2-4b4a-9043-718ce0a72a77",
   "metadata": {
    "id": "P7VKLnRkvZd4"
   },
   "outputs": [],
   "source": [
    "nlp_ru = stanza.Pipeline(lang='ru')\n",
    "nlp_en = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency')\n",
    "nlp_fr = stanza.Pipeline(lang='fr', processors='tokenize, mwt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb25d00-8822-4a7c-a286-a18b5a62d2ba",
   "metadata": {
    "id": "dUR7LXjmvtpn"
   },
   "source": [
    "Токенизация, сегментация по предложениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0db296cc-af5b-479b-a6a5-95863ba4f138",
   "metadata": {
    "executionInfo": {
     "elapsed": 2242,
     "status": "ok",
     "timestamp": 1663154958031,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "Db8CYc_SvtIX"
   },
   "outputs": [],
   "source": [
    "text = '''Архитектура Византии — совокупность традиций строительства и архитектуры в поздней Римской империи и в Византии в период с начала IV века по середину XV века. В качестве отдельных направлений исследования выделяют религиозную архитектуру Византии, византийскую фортификацию и гражданское строительство, включающее дворцы, общественные сооружения и частные дома. Также в рамках данной дисциплины изучают традиции строительного ремесла и декоративного искусства.'''\n",
    "\n",
    "doc = nlp_ru(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05394f74-5bd0-4f7a-ab87-d2f6529e079e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1663154817857,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "MMT0yrtGv3wP",
    "outputId": "2e4e3a15-9345-47c7-ed71-e4c8fc17dc2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Архитектура Византии — совокупность традиций строительства и архитектуры в поздней Римской империи и в Византии в период с начала IV века по середину XV века.\n",
      "В качестве отдельных направлений исследования выделяют религиозную архитектуру Византии, византийскую фортификацию и гражданское строительство, включающее дворцы, общественные сооружения и частные дома.\n",
      "Также в рамках данной дисциплины изучают традиции строительного ремесла и декоративного искусства.\n"
     ]
    }
   ],
   "source": [
    "print(*[sentence.text for sentence in doc.sentences], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4fb8c-d822-47ff-b71e-768c99bc933c",
   "metadata": {
    "id": "IxBb4XOMwJdf"
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "  print(f'====== Sentence {i + 1} tokens =======')\n",
    "  print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c465e86-a9e1-49cb-85a6-e5075489a80e",
   "metadata": {
    "id": "i2cRJqFSwO-v"
   },
   "outputs": [],
   "source": [
    "text_fr = '''Il est révélé par les romans Extension du domaine de la lutte (1994) et, surtout, Les Particules élémentaires (1998), qui le fait connaître d'un large public.'''\n",
    "\n",
    "doc_fr = nlp_fr(text_fr)\n",
    "for token in doc_fr.sentences[0].tokens:\n",
    "    print(f'token: {token.text}\\twords: {\", \".join([word.text for word in token.words])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6413c-f222-4d6a-bb3a-2d17887b8fe7",
   "metadata": {
    "id": "HlS7SKLUwetI"
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097f628-4bed-45e9-9629-8f0630f2f42b",
   "metadata": {
    "id": "Fh6HPJn2wgcp"
   },
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b93a3-400f-4056-8338-a39cd92cc707",
   "metadata": {
    "id": "5SsW-5MFwupQ"
   },
   "source": [
    "Морфопарсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f24da-5591-4cb9-ac66-8c1d6b0ed0e0",
   "metadata": {
    "id": "Z2wxiB8NwtYf"
   },
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24f916-7363-4487-9167-11180fe5fe43",
   "metadata": {
    "id": "bKZwX5l8wy6Q"
   },
   "source": [
    "Парсинг синтаксических зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6357fe-7e19-47ae-b8a1-30aca5f14406",
   "metadata": {
    "id": "w11Jqo6Fwx14"
   },
   "outputs": [],
   "source": [
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2e3bc-4e67-4964-8fa3-38c774c4b4ce",
   "metadata": {
    "id": "k7zTW3sJw27A"
   },
   "outputs": [],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e28e1-5dee-47b9-8285-e0c95d26395b",
   "metadata": {
    "id": "8-1cXBtWw5K4"
   },
   "source": [
    "Парсинг составляющих (для русского недоступен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e47277-7f7f-42d7-9500-337a3218095f",
   "metadata": {
    "id": "5KGb8sV6w6lo"
   },
   "outputs": [],
   "source": [
    "doc_en = nlp_en('This is a sentence for parsing constituencies.')\n",
    "\n",
    "for sentence in doc_en.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d50fb0-4e82-4cac-80bc-3a9cbb69cbaa",
   "metadata": {
    "id": "HWfmZei5t_8I"
   },
   "source": [
    "### natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bb30a-3d08-49d1-a9c2-37a8b6fd1c7b",
   "metadata": {
    "id": "Fw52LhAct9Tg"
   },
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112e0dd8-6eba-4e5d-8c69-66ff1e64493c",
   "metadata": {},
   "source": [
    "Natasha - сегментированная библиотека, которую можно использовать по частям. Самая востребованная часть библиотеки - сентенайзер и токенизатор, их можно и установить отдельно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f50af-bdf0-44d4-8791-fb86df4aced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "794a5741-fe1f-4d19-8916-b242b83cd493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Буллом-со (ммани, мандинги) — один из атлантических языков нигеро-конголезской макросемьи.',\n",
       " 'Распространён в прибрежных районах, возле границы между Гвинеей и Сьерра-Леоне.',\n",
       " 'По данным справочника Ethnologue число носителей составляет 8350 человек в Сьерра-Леоне и несколько человек в Гвинее, другие источники сообщают о гораздо меньшем количестве носителей (около 500 чел).',\n",
       " 'Наиболее близкородственный язык — бом, имеется небольшая взаимопонимаемость с шербро.',\n",
       " 'Буллом-со активно вытесняется соседними языками, главным образом — темне.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import sentenize\n",
    "\n",
    "raw = '''Буллом-со (ммани, мандинги) — один из атлантических языков нигеро-конголезской макросемьи. Распространён в прибрежных районах, возле границы между Гвинеей и Сьерра-Леоне. По данным справочника Ethnologue число носителей составляет 8350 человек в Сьерра-Леоне и несколько человек в Гвинее, другие источники сообщают о гораздо меньшем количестве носителей (около 500 чел). Наиболее близкородственный язык — бом, имеется небольшая взаимопонимаемость с шербро. Буллом-со активно вытесняется соседними языками, главным образом — темне.'''\n",
    "\n",
    "sents = [s.text for s in sentenize(raw)]\n",
    "\n",
    "sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeda31c-43f9-4897-916f-fd22091b0c1b",
   "metadata": {},
   "source": [
    "Для чего нам тут нужен генератор? Дело в том, что sentenize работает примерно как finditer: возвращает итератор из набора специальных объектов, очень похожих на Match из re, в которых содержится сам текст предложения + индексы его начала и конца в исходной строке. Обычно эти индексы никому не нужны, поэтому результат тут же пересобирывается в список строк. \n",
    "\n",
    "По такому же принципу устроен и токенизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec016bc3-bddc-4d5b-a4c1-1bdbafeef40b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Буллом-со',\n",
       " '(',\n",
       " 'ммани',\n",
       " ',',\n",
       " 'мандинги',\n",
       " ')',\n",
       " '—',\n",
       " 'один',\n",
       " 'из',\n",
       " 'атлантических',\n",
       " 'языков',\n",
       " 'нигеро-конголезской',\n",
       " 'макросемьи',\n",
       " '.',\n",
       " 'Распространён',\n",
       " 'в',\n",
       " 'прибрежных',\n",
       " 'районах',\n",
       " ',',\n",
       " 'возле',\n",
       " 'границы',\n",
       " 'между',\n",
       " 'Гвинеей',\n",
       " 'и',\n",
       " 'Сьерра-Леоне',\n",
       " '.',\n",
       " 'По',\n",
       " 'данным',\n",
       " 'справочника',\n",
       " 'Ethnologue',\n",
       " 'число',\n",
       " 'носителей',\n",
       " 'составляет',\n",
       " '8350',\n",
       " 'человек',\n",
       " 'в',\n",
       " 'Сьерра-Леоне',\n",
       " 'и',\n",
       " 'несколько',\n",
       " 'человек',\n",
       " 'в',\n",
       " 'Гвинее',\n",
       " ',',\n",
       " 'другие',\n",
       " 'источники',\n",
       " 'сообщают',\n",
       " 'о',\n",
       " 'гораздо',\n",
       " 'меньшем',\n",
       " 'количестве',\n",
       " 'носителей',\n",
       " '(',\n",
       " 'около',\n",
       " '500',\n",
       " 'чел',\n",
       " ')',\n",
       " '.',\n",
       " 'Наиболее',\n",
       " 'близкородственный',\n",
       " 'язык',\n",
       " '—',\n",
       " 'бом',\n",
       " ',',\n",
       " 'имеется',\n",
       " 'небольшая',\n",
       " 'взаимопонимаемость',\n",
       " 'с',\n",
       " 'шербро',\n",
       " '.',\n",
       " 'Буллом-со',\n",
       " 'активно',\n",
       " 'вытесняется',\n",
       " 'соседними',\n",
       " 'языками',\n",
       " ',',\n",
       " 'главным',\n",
       " 'образом',\n",
       " '—',\n",
       " 'темне',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "tokens = [t.text for t in tokenize(raw)]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7bebc-5a2a-441b-a4a3-08f522a5fa5f",
   "metadata": {
    "id": "8VqX_VxouI-q"
   },
   "source": [
    "Морфосинтаксический парсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b8550a8-53f1-44a1-a650-be5a38981486",
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1663154496060,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "S5k9Wlz9uK7M"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    \n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c6338c-05dd-489d-ab0b-75b46b7e4ca0",
   "metadata": {
    "id": "ciOkGldHuLgB"
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter()  # токенизация и разделение на предложения\n",
    "emb = NewsEmbedding()  # эмбеддинги\n",
    "morph_tagger = NewsMorphTagger(emb)  # морфология\n",
    "syntax_parser = NewsSyntaxParser(emb) # синтаксис\n",
    "\n",
    "text = '29 марта 2017 года правительство Великобритании инициировало процедуру выхода в соответствии со статьёй 50 Договора о Европейском союзе; первоначально планировалось, что Великобритания покинет Европейский союз через два года, 29 марта 2019 года в 23:00 по Гринвичу.'\n",
    "doc = Doc(text)\n",
    "\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "sent = doc.sents[0]\n",
    "sent.morph.print()\n",
    "sent.syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b78b020-beb1-4ac8-87e6-1b250186cd8f",
   "metadata": {
    "id": "YPCVWdB1ujFp"
   },
   "source": [
    "Распознание именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b51081-e41e-47bc-bbbf-b6da0d452661",
   "metadata": {
    "id": "AaTUTjyMuikI"
   },
   "outputs": [],
   "source": [
    "from natasha import NewsNERTagger\n",
    "\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd399b37-b337-4b53-9df7-a49cc734d226",
   "metadata": {
    "id": "Log6uopyux7n"
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c95d9d9-13b9-4410-b87b-3c5bde587c56",
   "metadata": {
    "id": "RNSGZViPuuJd"
   },
   "outputs": [],
   "source": [
    "from natasha import MorphVocab\n",
    "\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "for token in doc.tokens:\n",
    "  token.lemmatize(morph_vocab)\n",
    "  print(token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212ba8a-0f27-4946-ab09-7efddbe9be45",
   "metadata": {
    "id": "o1dqpS_Hu3OT"
   },
   "source": [
    "Нормализация именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca10a7b-d4d3-48ca-baad-47f2c80d1b27",
   "metadata": {
    "id": "gqmxRFqbu2m8"
   },
   "outputs": [],
   "source": [
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "   \n",
    "{_.text: _.normal for _ in doc.spans}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
